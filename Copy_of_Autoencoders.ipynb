{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Autoencoders",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataMonsterBoy/dtw/blob/master/Copy_of_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rI0tCFK2xeoU",
        "colab_type": "code",
        "outputId": "cc3034a4-09db-4761-9b86-f8732fba689b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "# This tutorial was taken from https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "if \"COLAB_TPU_ADDR\" not in os.environ:\n",
        " print(\"ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!\")\n",
        "else:\n",
        " tpu_address = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"]\n",
        " print (\"TPU address is\", tpu_address)\n",
        "with tf.Session(tpu_address) as session:\n",
        " devices = session.list_devices()\n",
        " \n",
        " print(\"TPU devices:\")\n",
        " pprint.pprint(devices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.120.119.66:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 997373557158828602),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8875429238357423198),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5764581782689679828),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3526425674130694302),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10385598250267310612),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10623468659183554437),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 10339065335251603162),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14426321889355046676),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6912304788773976631),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15234987434574802184),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6256754587511949882)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-7w4IaWvyBbG",
        "colab_type": "code",
        "outputId": "6c78267c-d8c3-4469-e527-d71266acab8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# this is the size of our encoded representations\n",
        "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
        "\n",
        "# this is our input placeholder\n",
        "input_img = Input(shape=(784,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "w6XAE8yCyExw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iH8VGNqKyY-w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s2n7PRmozS2f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KCrLyA5EyaC_",
        "colab_type": "code",
        "outputId": "cb19e8e0-1d11-47f7-baa5-4f6d7b1634d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "# minst.load_data()is a 2d array where the first element is the train and the second is the test. In the train the first elemnt \n",
        "# are the pixels and the second is the target\n",
        "_training = mnist.load_data()[0] # this is the training data\n",
        "_test = mnist.load_data()[1] # this is the test data\n",
        "_training_pixels = mnist.load_data()[0][0] # this is the training pixel data\n",
        "_test_pixels = mnist.load_data()[1][0] # this is the test pixel data\n",
        "_training_target = mnist.load_data()[0][1] # this is the training target data\n",
        "_test_target = mnist.load_data()[1][1] # this is the test target data\n",
        "\n",
        "# read in training and test. discard target as we are constructing an autoencoder\n",
        "(x_train, _), (x_test, _) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ec0C6g4By_d5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WE-F_jSfytQp",
        "colab_type": "code",
        "outputId": "cdbc8130-81cf-41fd-b6f6-5c4152f459c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1822
        }
      },
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.3603 - val_loss: 0.2712\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2641 - val_loss: 0.2530\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.2425 - val_loss: 0.2296\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2221 - val_loss: 0.2121\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2073 - val_loss: 0.1999\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.1969 - val_loss: 0.1911\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.1890 - val_loss: 0.1841\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.1825 - val_loss: 0.1780\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1767 - val_loss: 0.1726\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1714 - val_loss: 0.1676\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.1666 - val_loss: 0.1630\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1623 - val_loss: 0.1588\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1583 - val_loss: 0.1550\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1547 - val_loss: 0.1516\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1513 - val_loss: 0.1486\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1483 - val_loss: 0.1455\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.1456 - val_loss: 0.1428\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.1431 - val_loss: 0.1404\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1407 - val_loss: 0.1382\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1385 - val_loss: 0.1361\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1364 - val_loss: 0.1339\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1344 - val_loss: 0.1319\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.1325 - val_loss: 0.1301\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.1307 - val_loss: 0.1283\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.1289 - val_loss: 0.1265\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1272 - val_loss: 0.1248\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1255 - val_loss: 0.1232\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1240 - val_loss: 0.1217\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1225 - val_loss: 0.1202\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.1211 - val_loss: 0.1190\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.1198 - val_loss: 0.1176\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1186 - val_loss: 0.1164\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1175 - val_loss: 0.1153\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.1164 - val_loss: 0.1143\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1154 - val_loss: 0.1133\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.1145 - val_loss: 0.1124\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.1137 - val_loss: 0.1116\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.1129 - val_loss: 0.1108\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.1122 - val_loss: 0.1101\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.1115 - val_loss: 0.1095\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1108 - val_loss: 0.1089\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1103 - val_loss: 0.1083\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1097 - val_loss: 0.1078\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1092 - val_loss: 0.1073\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1087 - val_loss: 0.1068\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1083 - val_loss: 0.1064\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1078 - val_loss: 0.1059\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.1074 - val_loss: 0.1055\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1070 - val_loss: 0.1052\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1067 - val_loss: 0.1048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feb20cbc208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "WakqN5Qjy30e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QI2C3Boh2Yrt",
        "colab_type": "code",
        "outputId": "742dd2ce-8367-4f4a-d107-447449f22d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "cell_type": "code",
      "source": [
        "# use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQfCgBgAAAAAAoCD+HxSj78eVr+VNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "P57NmFEb2pKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# variational autoencoders\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "\n",
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "def plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=128,\n",
        "                 model_name=\"vae_mnist\"):\n",
        "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
        "    # Arguments\n",
        "        models (tuple): encoder and decoder models\n",
        "        data (tuple): test data and label\n",
        "        batch_size (int): prediction batch size\n",
        "        model_name (string): which model is using this function\n",
        "    \"\"\"\n",
        "\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "    os.makedirs(model_name, exist_ok=True)\n",
        "\n",
        "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
        "    # display a 30x30 2D manifold of digits\n",
        "    n = 30\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-4, 4, n)\n",
        "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.savefig(filename)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUALDyl69xkU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "original_dim = image_size * image_size\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XNEr-FrD-8w6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# network parameters\n",
        "input_shape = (original_dim, )\n",
        "intermediate_dim = 512\n",
        "batch_size = 128\n",
        "latent_dim = 2\n",
        "epochs = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PZ5PjTGM-iQ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zsTru6-K_mst",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FBMIkuFt_neu",
        "colab_type": "code",
        "outputId": "6667895d-4223-4109-88ad-0be0dbbe0a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 512)          401920      encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 2)            1026        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 2)            1026        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 403,972\n",
            "Trainable params: 403,972\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RKdyDTo_BSYZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = Dense(original_dim, activation='sigmoid')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ogSDHh_iCBdE",
        "colab_type": "code",
        "outputId": "6d989106-14fb-4263-f3a5-933cb63adeb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "z_sampling (InputLayer)      (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               1536      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 784)               402192    \n",
            "=================================================================\n",
            "Total params: 403,728\n",
            "Trainable params: 403,728\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4sf8RagYCJng",
        "colab_type": "code",
        "outputId": "7e38cca1-861f-4eb4-98f6-91b8fc411f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['-m', '--mse'], dest='mse', nargs=0, const=True, default=False, type=None, choices=None, help='Use mse loss instead of binary cross entropy (default)', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "jeWsWG29CQWr",
        "colab_type": "code",
        "outputId": "7722403c-1b3c-46cf-8e63-fe992263ae36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "cell_type": "code",
      "source": [
        "models = (encoder, decoder)\n",
        "data = (x_test, y_test)\n",
        "\n",
        "# VAE loss = mse_loss or xent_loss + kl_loss\n",
        "#if args.mse:\n",
        "    #reconstruction_loss = mse(inputs, outputs)\n",
        "#else:\n",
        "reconstruction_loss = binary_crossentropy(inputs,\n",
        "                                              outputs)\n",
        "\n",
        "reconstruction_loss *= original_dim\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "vae.summary()\n",
        "plot_model(vae,\n",
        "           to_file='vae_mlp.png',\n",
        "           show_shapes=True)\n",
        "\n",
        "#if args.weights:\n",
        "    #vae.load_weights(args.weights)\n",
        "#else:\n",
        "    # train the autoencoder\n",
        "vae.fit(x_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test, None))\n",
        "vae.save_weights('vae_mlp_mnist.h5')\n",
        "\n",
        "plot_results(models,\n",
        "             data,\n",
        "             batch_size=batch_size,\n",
        "             model_name=\"vae_mlp\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              [(None, 2), (None, 2), (N 403972    \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 784)               403728    \n",
            "=================================================================\n",
            "Total params: 807,700\n",
            "Trainable params: 807,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 392.7785 - val_loss: 343.2205\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 20s 328us/step - loss: 338.3307 - val_loss: 334.3186\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 20s 326us/step - loss: 330.4104 - val_loss: 328.0664\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 324.6870 - val_loss: 322.9037\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 320.3209 - val_loss: 318.8856\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 20s 328us/step - loss: 317.1104 - val_loss: 316.5052\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 314.7649 - val_loss: 315.1462\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 312.9698 - val_loss: 312.9719\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 311.4375 - val_loss: 311.5046\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 310.1624 - val_loss: 311.0619\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 20s 326us/step - loss: 309.0908 - val_loss: 310.1231\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 20s 328us/step - loss: 308.1451 - val_loss: 309.6575\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 307.3474 - val_loss: 308.3091\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 306.5364 - val_loss: 308.3187\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 20s 328us/step - loss: 305.7175 - val_loss: 307.2438\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 20s 328us/step - loss: 305.0390 - val_loss: 307.1740\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 20s 331us/step - loss: 304.5086 - val_loss: 306.2218\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 20s 333us/step - loss: 303.8997 - val_loss: 305.9779\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 20s 332us/step - loss: 303.3269 - val_loss: 305.5319\n",
            "Epoch 20/50\n",
            "28672/60000 [=============>................] - ETA: 10s - loss: 302.3326"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A5kTCtF1OTMU",
        "colab_type": "code",
        "outputId": "0f736378-87a3-4108-da93-21f2f40830f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-67ecdacf-173c-4ba7-ae3c-efe730b17551.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "iqihs2N7EHmG",
        "colab_type": "code",
        "outputId": "43b34a85-2de9-4c1e-8016-cf5d8e6648b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3e1edd6a2952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1746\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1747\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2402\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "H5F-H4lLObgE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}